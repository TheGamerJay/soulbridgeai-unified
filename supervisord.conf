[supervisord]
nodaemon=true
logfile=/tmp/supervisord.log
logfile_maxbytes=50MB
logfile_backups=10
loglevel=info
user=root
environment=OLLAMA_HOST=0.0.0.0:11434,OLLAMA_KEEP_ALIVE=24h,FREE_MODEL=tinyllama,PORT=8080,OLLAMA_BASE=http://127.0.0.1:11434

[program:ollama]
command=/bin/bash -lc "ollama serve"
priority=10
autostart=true
autorestart=true
stdout_logfile=/tmp/ollama.log
stderr_logfile=/tmp/ollama.log
startsecs=2

[program:preload]
command=/bin/bash -lc "set -euo pipefail; echo '[preload] waiting for Ollama...'; until curl -sf http://127.0.0.1:11434/api/tags >/dev/null; do echo '[preload] still waiting...'; sleep 1; done; echo '[preload] Ollama up; pulling model ' ${FREE_MODEL:-tinyllama}; (ollama pull ${FREE_MODEL:-tinyllama} || true); echo '[preload] warming via HTTP'; curl -sf -X POST http://127.0.0.1:11434/api/chat -H 'Content-Type: application/json' -d '{\"model\":\"'${FREE_MODEL:-tinyllama}'\",\"messages\":[{\"role\":\"user\",\"content\":\"say hi\"}]}' > /tmp/preload_warmup.json || true; if grep -q 'message' /tmp/preload_warmup.json 2>/dev/null; then echo '[preload] ✅ warmup OK'; else echo '[preload] ⚠ warmup response did not include a message; dumping:'; sed -n '1,120p' /tmp/preload_warmup.json || true; fi; echo '[preload] done'"
priority=20
autostart=true
autorestart=false
stdout_logfile=/tmp/preload.log
stderr_logfile=/tmp/preload.log

[program:web]
command=/bin/bash -lc "echo 'waiting for ollama before starting web...'; until curl -sf http://127.0.0.1:11434/api/tags >/dev/null; do sleep 1; done; cd /app && ./start.sh"
priority=30
autostart=true
autorestart=true
stdout_logfile=/tmp/web.log
stderr_logfile=/tmp/web.log
startsecs=2