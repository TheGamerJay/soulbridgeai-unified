[supervisord]
nodaemon=true
logfile=/tmp/supervisord.log
logfile_maxbytes=50MB
logfile_backups=10
loglevel=info
user=root
environment=OLLAMA_HOST=0.0.0.0:11434,OLLAMA_KEEP_ALIVE=24h,FREE_MODEL=tinyllama,PORT=8080,OLLAMA_BASE=http://127.0.0.1:11434

[program:ollama]
command=/bin/bash -lc "ollama serve"
priority=10
autostart=true
autorestart=true
stdout_logfile=/tmp/ollama.log
stderr_logfile=/tmp/ollama.log
startsecs=2

[program:preload]
command=/bin/bash -lc "echo 'waiting for ollama...'; until curl -sf http://127.0.0.1:11434/api/tags >/dev/null; do sleep 1; done; echo 'ollama up; pulling model ' ${FREE_MODEL:-tinyllama}; (ollama pull ${FREE_MODEL:-tinyllama} || true); echo 'warming via HTTP'; curl -sf -X POST http://127.0.0.1:11434/api/chat -H 'Content-Type: application/json' -d '{\"model\":\"'${FREE_MODEL:-tinyllama}'\",\"messages\":[{\"role\":\"user\",\"content\":\"hi\"}]}' >/tmp/preload_warmup.json || true; echo ${FREE_MODEL:-tinyllama}' preload and warmup done'"
priority=20
autostart=true
autorestart=false
stdout_logfile=/tmp/preload.log
stderr_logfile=/tmp/preload.log

[program:web]
command=/bin/bash -lc "echo 'waiting for ollama before starting web...'; until curl -sf http://127.0.0.1:11434/api/tags >/dev/null; do sleep 1; done; cd /app && ./start.sh"
priority=30
autostart=true
autorestart=true
stdout_logfile=/tmp/web.log
stderr_logfile=/tmp/web.log
startsecs=2