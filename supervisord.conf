[supervisord]
nodaemon=true
logfile=/tmp/supervisord.log
logfile_maxbytes=50MB
logfile_backups=10
loglevel=info
user=root
environment=OLLAMA_HOST=0.0.0.0:11434,OLLAMA_KEEP_ALIVE=24h,FREE_MODEL=tinyllama,PORT=8080,OLLAMA_BASE=http://127.0.0.1:11434

[program:ollama]
command=/bin/bash -lc "ollama serve"
priority=10
autostart=true
autorestart=true
stdout_logfile=/tmp/ollama.log
stderr_logfile=/tmp/ollama.log
startsecs=2

[program:preload]
command=/bin/bash -lc "set -euo pipefail; echo '[preload] waiting for Ollama...'; until curl -sf http://127.0.0.1:11434/api/tags >/dev/null; do echo '[preload] still waiting...'; sleep 1; done; echo '[preload] Ollama up; pulling model ' ${FREE_MODEL:-tinyllama}; (ollama pull ${FREE_MODEL:-tinyllama} || true); echo '[preload] warming via HTTP with 60s timeout...'; timeout 60 curl -sf -X POST http://127.0.0.1:11434/api/chat -H 'Content-Type: application/json' -d '{\"model\":\"'${FREE_MODEL:-tinyllama}'\",\"messages\":[{\"role\":\"user\",\"content\":\"say hi\"}]}' > /tmp/preload_warmup.json || echo '[preload] warmup timed out after 60s'; if grep -q 'message' /tmp/preload_warmup.json 2>/dev/null; then echo '[preload] ✅ warmup OK'; else echo '[preload] ⚠ warmup response issue; dumping:'; head -20 /tmp/preload_warmup.json || true; fi; echo '[preload] done'; touch /tmp/preload_complete"
priority=20
autostart=true
autorestart=false
stdout_logfile=/tmp/preload.log
stderr_logfile=/tmp/preload.log

[program:web]
command=/bin/bash -lc "echo '[web] waiting for preload to complete...'; until [ -f /tmp/preload_complete ]; do echo '[web] still waiting for preload...'; sleep 3; done; echo '[web] preload done, final ollama check...'; until curl -sf http://127.0.0.1:11434/api/tags >/dev/null; do echo '[web] ollama not ready...'; sleep 1; done; echo '[web] starting web server with gthread worker'; cd /app && ./start.sh"
priority=30
autostart=true
autorestart=true
stdout_logfile=/tmp/web.log
stderr_logfile=/tmp/web.log
startsecs=2